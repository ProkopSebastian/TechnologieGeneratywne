{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "@dataclass\n",
    "class Product:\n",
    "    name: str\n",
    "    price: str\n",
    "    original_price: Optional[str] = None\n",
    "    discount_info: Optional[str] = None\n",
    "    unit: Optional[str] = None\n",
    "    promotion_type: Optional[str] = None\n",
    "    product_url: Optional[str] = None\n",
    "    image_url: Optional[str] = None\n",
    "\n",
    "class BiedronkaScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'pl-PL,pl;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        self.base_url = \"https://www.biedronka.pl\"\n",
    "    \n",
    "    def fetch_page(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Pobiera stronę i zwraca obiekt BeautifulSoup\"\"\"\n",
    "        try:\n",
    "            print(f\"Pobieranie: {url}\")\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Błąd podczas pobierania strony {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_product_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:\n",
    "        \"\"\"Wyciąga linki do produktów ze strony głównej\"\"\"\n",
    "        product_links = []\n",
    "        \n",
    "        # Szuka linków do produktów\n",
    "        links = soup.find_all('a', href=re.compile(r'/pl/product,id,\\d+'))\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            title = link.get('title', '')\n",
    "            \n",
    "            # Sprawdza czy link ma obrazek (oznacza że to główny link produktu)\n",
    "            img = link.find('img')\n",
    "            if img and href and title:\n",
    "                product_links.append({\n",
    "                    'url': urljoin(self.base_url, href),\n",
    "                    'name': title.strip(),\n",
    "                    'image_url': img.get('src', '') if img else ''\n",
    "                })\n",
    "        \n",
    "        # Usuwa duplikaty\n",
    "        seen_urls = set()\n",
    "        unique_links = []\n",
    "        for link in product_links:\n",
    "            if link['url'] not in seen_urls:\n",
    "                seen_urls.add(link['url'])\n",
    "                unique_links.append(link)\n",
    "        \n",
    "        return unique_links\n",
    "    \n",
    "    def extract_price_from_product_page(self, soup: BeautifulSoup) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"Wyciąga informacje o cenie ze strony produktu\"\"\"\n",
    "        price_info = {\n",
    "            'price': None,\n",
    "            'original_price': None,\n",
    "            'discount_info': None,\n",
    "            'unit': None,\n",
    "            'promotion_type': None\n",
    "        }\n",
    "        \n",
    "        # Szuka głównej ceny - najpierw próbuje konkretnych klas\n",
    "        pln_elem = soup.select_one('.pln')\n",
    "        gr_elem = soup.select_one('.gr')\n",
    "        \n",
    "        if pln_elem:\n",
    "            pln_text = pln_elem.get_text().strip()\n",
    "            if gr_elem:\n",
    "                gr_text = gr_elem.get_text().strip()\n",
    "                price_info['price'] = f\"{pln_text},{gr_text}\"\n",
    "            else:\n",
    "                price_info['price'] = pln_text\n",
    "        \n",
    "        # Fallback - szuka innych selektorów jeśli nie ma klas pln/gr\n",
    "        if not price_info['price']:\n",
    "            price_selectors = [\n",
    "                '.price-wrapper .price',\n",
    "                '.price-item .price',\n",
    "                '.product-price .price',\n",
    "                '[class*=\"price\"]:not([class*=\"original\"]):not([class*=\"old\"])',\n",
    "                '.price-current',\n",
    "                '.current-price'\n",
    "            ]\n",
    "            \n",
    "            for selector in price_selectors:\n",
    "                price_elem = soup.select_one(selector)\n",
    "                if price_elem:\n",
    "                    price_text = price_elem.get_text().strip()\n",
    "                    # Wyciąga cenę (liczby z przecinkami/kropkami i groszami)\n",
    "                    price_match = re.search(r'(\\d+[,.]?\\d{0,2})', price_text)\n",
    "                    if price_match:\n",
    "                        price_info['price'] = price_match.group(1)\n",
    "                        break\n",
    "        \n",
    "        # Alternatywna metoda - szuka w tekście\n",
    "        if not price_info['price']:\n",
    "            # Szuka wzorców cenowych w całym tekście\n",
    "            text_content = soup.get_text()\n",
    "            price_patterns = [\n",
    "                r'(\\d{1,3}[,.]?\\d{0,2})\\s*zł',\n",
    "                r'cena[:\\s]+(\\d{1,3}[,.]?\\d{0,2})',\n",
    "                r'(\\d{1,3}[,.]?\\d{0,2})\\s*/\\s*(?:kg|szt|opak|l)',\n",
    "            ]\n",
    "            \n",
    "            for pattern in price_patterns:\n",
    "                match = re.search(pattern, text_content, re.IGNORECASE)\n",
    "                if match:\n",
    "                    price_info['price'] = match.group(1)\n",
    "                    break\n",
    "        \n",
    "        # Szuka oryginalnej ceny\n",
    "        original_price_selectors = [\n",
    "            '.price-original',\n",
    "            '.old-price',\n",
    "            '.price-before',\n",
    "            '[class*=\"original\"]',\n",
    "            '.crossed-price'\n",
    "        ]\n",
    "        \n",
    "        for selector in original_price_selectors:\n",
    "            orig_elem = soup.select_one(selector)\n",
    "            if orig_elem:\n",
    "                orig_text = orig_elem.get_text().strip()\n",
    "                orig_match = re.search(r'(\\d+[,.]?\\d{0,2})', orig_text)\n",
    "                if orig_match:\n",
    "                    price_info['original_price'] = orig_match.group(1)\n",
    "                    break\n",
    "        \n",
    "        # Szuka informacji o promocji\n",
    "        promo_text = soup.get_text().lower()\n",
    "        \n",
    "        if 'gratis' in promo_text and ('1+1' in promo_text or 'drugi' in promo_text):\n",
    "            price_info['promotion_type'] = '1+1 GRATIS'\n",
    "        elif 'drugi' in promo_text and 'taniej' in promo_text:\n",
    "            # Szuka procentu zniżki\n",
    "            discount_match = re.search(r'drugi[^0-9]*(\\d+)%[^a-z]*taniej', promo_text)\n",
    "            if discount_match:\n",
    "                price_info['promotion_type'] = f'Drugi {discount_match.group(1)}% taniej'\n",
    "                price_info['discount_info'] = f'{discount_match.group(1)}% taniej na drugi produkt'\n",
    "        elif 'supercena' in promo_text:\n",
    "            price_info['promotion_type'] = 'SUPERCENA'\n",
    "        \n",
    "        # Szuka ogólnych zniżek\n",
    "        discount_match = re.search(r'(\\d+)%\\s*taniej', promo_text)\n",
    "        if discount_match and not price_info['discount_info']:\n",
    "            price_info['discount_info'] = f'{discount_match.group(1)}% taniej'\n",
    "        \n",
    "        # Szuka jednostki\n",
    "        unit_match = re.search(r'/\\s*(kg|szt|opak|l|ml|g)\\b', soup.get_text())\n",
    "        if unit_match:\n",
    "            price_info['unit'] = unit_match.group(1)\n",
    "        \n",
    "        return price_info\n",
    "    \n",
    "    def scrape_product_details(self, product_link: Dict[str, str]) -> Optional[Product]:\n",
    "        \"\"\"Scrapuje szczegóły pojedynczego produktu\"\"\"\n",
    "        soup = self.fetch_page(product_link['url'])\n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        # Wyciąga informacje o cenie\n",
    "        price_info = self.extract_price_from_product_page(soup)\n",
    "        \n",
    "        # Tworzy obiekt produktu\n",
    "        product = Product(\n",
    "            name=product_link['name'],\n",
    "            price=price_info['price'] or 'Sprawdź w sklepie',\n",
    "            original_price=price_info['original_price'],\n",
    "            discount_info=price_info['discount_info'],\n",
    "            unit=price_info['unit'],\n",
    "            promotion_type=price_info['promotion_type'],\n",
    "            product_url=product_link['url'],\n",
    "            image_url=product_link['image_url']\n",
    "        )\n",
    "        \n",
    "        return product\n",
    "    \n",
    "    def scrape_offers(self, url: str, max_products: int = 20) -> List[Product]:\n",
    "        \"\"\"Główna metoda do scrapowania ofert\"\"\"\n",
    "        print(f\"Scrapowanie ofert z: {url}\")\n",
    "        \n",
    "        # Pobiera stronę główną\n",
    "        soup = self.fetch_page(url)\n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        # Wyciąga linki do produktów\n",
    "        product_links = self.extract_product_links(soup)\n",
    "        print(f\"Znaleziono {len(product_links)} linków do produktów\")\n",
    "        \n",
    "        if not product_links:\n",
    "            print(\"Nie znaleziono linków do produktów. Sprawdzam strukturę strony...\")\n",
    "            # Debug - pokazuje fragment HTML\n",
    "            print(\"Pierwsze 1000 znaków HTML:\")\n",
    "            print(soup.prettify()[:1000])\n",
    "            return []\n",
    "        \n",
    "        # Ogranicza liczbę produktów do sprawdzenia\n",
    "        products_to_check = product_links[:max_products]\n",
    "        print(f\"Sprawdzanie szczegółów dla {len(products_to_check)} produktów...\")\n",
    "        \n",
    "        products = []\n",
    "        for i, product_link in enumerate(products_to_check, 1):\n",
    "            print(f\"Sprawdzanie produktu {i}/{len(products_to_check)}: {product_link['name']}\")\n",
    "            \n",
    "            product = self.scrape_product_details(product_link)\n",
    "            if product:\n",
    "                products.append(product)\n",
    "            \n",
    "            # Przerwa między requestami\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"Pomyślnie wyciągnięto dane dla {len(products)} produktów\")\n",
    "        return products\n",
    "    \n",
    "    def save_to_json(self, products: List[Product], filename: str = \"biedronka_offers.json\"):\n",
    "        \"\"\"Zapisuje produkty do pliku JSON\"\"\"\n",
    "        products_dict = []\n",
    "        for product in products:\n",
    "            products_dict.append({\n",
    "                'name': product.name,\n",
    "                'price': product.price,\n",
    "                'original_price': product.original_price,\n",
    "                'discount_info': product.discount_info,\n",
    "                'unit': product.unit,\n",
    "                'promotion_type': product.promotion_type,\n",
    "                'product_url': product.product_url,\n",
    "                'image_url': product.image_url\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(products_dict, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Dane zapisane do pliku: {filename}\")\n",
    "    \n",
    "    def print_products(self, products: List[Product]):\n",
    "        \"\"\"Wyświetla produkty w konsoli\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OFERTY BIEDRONKA - KARTA MOJA BIEDRONKA\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, product in enumerate(products, 1):\n",
    "            print(f\"\\n{i}. {product.name}\")\n",
    "            print(f\"   Cena: {product.price}\")\n",
    "            \n",
    "            if product.original_price:\n",
    "                print(f\"   Cena oryginalna: {product.original_price} zł\")\n",
    "            \n",
    "            if product.unit:\n",
    "                print(f\"   Jednostka: /{product.unit}\")\n",
    "            \n",
    "            if product.promotion_type:\n",
    "                print(f\"   Promocja: {product.promotion_type}\")\n",
    "            \n",
    "            if product.discount_info:\n",
    "                print(f\"   Zniżka: {product.discount_info}\")\n",
    "            \n",
    "            if product.product_url:\n",
    "                print(f\"   Link: {product.product_url}\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Główna funkcja programu\"\"\"\n",
    "    scraper = BiedronkaScraper()\n",
    "    \n",
    "    # URL do scrapowania\n",
    "    url = \"https://www.biedronka.pl/pl/oferta-z-karta-moja-biedronka\"\n",
    "    \n",
    "    print(\"=== BIEDRONKA SCRAPER ===\")\n",
    "    print(\"Ten scraper będzie:\")\n",
    "    print(\"1. Znajdować linki do produktów na stronie głównej\")\n",
    "    print(\"2. Wchodzić w każdy link i wyciągać szczegóły\")\n",
    "    print(\"3. Zapisywać wyniki do JSON\")\n",
    "    print()\n",
    "    \n",
    "    # Pyta o liczbę produktów do sprawdzenia\n",
    "    try:\n",
    "        max_products = int(input(\"Ile produktów sprawdzić? (domyślnie 10): \") or \"10\")\n",
    "    except ValueError:\n",
    "        max_products = 10\n",
    "    \n",
    "    print(f\"\\nRozpoczynanie scrapowania dla {max_products} produktów...\")\n",
    "    \n",
    "    # Scrapuje oferty\n",
    "    products = scraper.scrape_offers(url, max_products)\n",
    "    \n",
    "    if products:\n",
    "        # Wyświetla produkty\n",
    "        scraper.print_products(products)\n",
    "        \n",
    "        # Zapisuje do JSON\n",
    "        scraper.save_to_json(products)\n",
    "        \n",
    "        print(f\"\\n\\nPodsumowanie:\")\n",
    "        print(f\"- Znaleziono {len(products)} produktów\")\n",
    "        print(f\"- Dane zapisane do pliku biedronka_offers.json\")\n",
    "    else:\n",
    "        print(\"Nie udało się wyciągnąć żadnych produktów.\")\n",
    "        print(\"Możliwe przyczyny:\")\n",
    "        print(\"- Strona zmieniła strukturę\")\n",
    "        print(\"- Problemy z połączeniem\")\n",
    "        print(\"- Blokada scrapera przez serwer\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
